{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling in missing data of CO$_2$ time series.\n",
    "\n",
    "This notebook fills in the missing values using ETS models and creates new .csv files with the updated information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ast\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from math import sqrt\n",
    "from datetime import *\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathr='/home/_ehoyos/Documents/Data_CO2/Final_info/' # path to read the data.\n",
    "paths='/home/_ehoyos/Documents/Data_CO2/Final_results/' # path to save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to select the best model\n",
    "\n",
    "def exp_smoothing_forecast(history,config):\n",
    "    t,d,s,p,r=config\n",
    "    # define model\n",
    "    history=array(history)\n",
    "    model=ExponentialSmoothing(history,trend=t,damped_trend=d,seasonal=s,seasonal_periods=p)\n",
    "    # fit model\n",
    "    model_fit=model.fit(optimized=True,remove_bias=r)\n",
    "    yhat=model_fit.predict(0,len(history)-1)  \n",
    "    # estimate prediction error\n",
    "    error,r2=measure_rmse(history,yhat)\n",
    "    return (error,r2,model_fit.params[\"smoothing_level\"],model_fit.params[\"smoothing_trend\"],\n",
    "           model_fit.params[\"smoothing_seasonal\"],model_fit.params[\"damping_trend\"])\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    error=sqrt(mean_squared_error(actual, predicted))\n",
    "    r2=metrics.r2_score(actual,predicted) \n",
    "    return (error,r2)\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(dataf,n_test):\n",
    "    return dataf,dataf \n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(dataf,n_test,cfg):\n",
    "    # split dataset\n",
    "    train,test=train_test_split(dataf,n_test)\n",
    "    # seed history with training dataset\n",
    "    history=[x for x in train]\n",
    "    error,r2,a1,b1,g1,f1=exp_smoothing_forecast(history,cfg) \n",
    "    return (error,r2,a1,b1,g1,f1)\n",
    "\n",
    "# score a model, return None on failure\n",
    "def score_model(dataf,n_test,cfg,debug=False):\n",
    "    result=None\n",
    "    r2=None\n",
    "    aa=None;bb=None;gg=None;ff=None #values of alpha, beta, gamma and phi.\n",
    "    # convert config to a key\n",
    "    key=str(cfg)\n",
    "    # show all warnings and fail on exception if debugging\n",
    "    if debug:\n",
    "        result,r2,aa,bb,gg,ff=walk_forward_validation(dataf,n_test,cfg)\n",
    "    else:\n",
    "        # one failure during model validation suggests an unstable config\n",
    "        try:\n",
    "            # never show warnings when grid searching, too noisy\n",
    "            with catch_warnings():\n",
    "                filterwarnings(\"ignore\")\n",
    "                result,r2,aa,bb,gg,ff=walk_forward_validation(dataf,n_test,cfg)\n",
    "        except:\n",
    "            error=None\n",
    "    return (key,result,r2,aa,bb,gg,ff)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search(dataf,cfg_list,n_test,parallel=True):\n",
    "    scores=None\n",
    "    if parallel:\n",
    "        # execute configs in parallel\n",
    "        executor=Parallel(n_jobs=cpu_count(),backend='multiprocessing')\n",
    "        tasks=(delayed(score_model)(dataf,n_test,cfg) for cfg in cfg_list)\n",
    "        scores=executor(tasks)\n",
    "    else:\n",
    "        scores=[score_model(dataf,n_test,cfg) for cfg in cfg_list]\n",
    "    # remove empty results\n",
    "    scores=[r for r in scores if r[1] !=None]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup:tup[1])\n",
    "    return scores\n",
    "\n",
    "# create a set of exponential smoothing configs to try\n",
    "def exp_smoothing_configs(seasonal=[None]):\n",
    "    models=list()\n",
    "    # define config lists\n",
    "    t_params=['add','mul',None] # trend.\n",
    "    d_params=[True,False] # damped trend.\n",
    "    s_params=['add','mul',None] # seasonal.\n",
    "    p_params=seasonal # seasonal period.\n",
    "    r_params=[True,False] # remove bias.\n",
    "    # create config instances\n",
    "    for t in t_params:\n",
    "        for d in d_params:\n",
    "            for s in s_params:\n",
    "                for p in p_params:\n",
    "                    for r in r_params:\n",
    "                        cfg=[t,d,s,p,r]\n",
    "                        models.append(cfg)\n",
    "    return models\n",
    "\n",
    "### Functions to fill in the missing values after selecting the best model\n",
    "\n",
    "def exp_smoothing_forecast_f(history,config):\n",
    "    t,d,s,p,r=config\n",
    "    history=array(history)\n",
    "    model=ExponentialSmoothing(history,trend=t,damped_trend=d,seasonal=s,seasonal_periods=p)\n",
    "    model_fit=model.fit(optimized=True,remove_bias=r)\n",
    "    yhat=model_fit.predict(len(history),len(history))\n",
    "    return yhat[0]\n",
    "\n",
    "def train_test_split_f(dataf,n_test):\n",
    "    return dataf[:-n_test],dataf[-n_test:] \n",
    "\n",
    "def walk_forward_validation_f(dataf,n_test,cfg):\n",
    "    predictions=list()\n",
    "    train,test=train_test_split_f(dataf,n_test)\n",
    "    history=[x for x in train]\n",
    "    for i in range(n_test): \n",
    "        yhat=exp_smoothing_forecast_f(history,cfg)\n",
    "        warnings.filterwarnings('ignore')\n",
    "        predictions.append(yhat)\n",
    "        history.append(yhat) \n",
    "    return history\n",
    "\n",
    "def ci_additive(dataf,config,n_test):\n",
    "    t,d,s,p,r=config\n",
    "    ci_lower=list(); ci_upper=list()\n",
    "    ets_model=sm.tsa.statespace.ExponentialSmoothing(dataf,trend=t,damped_trend=d,seasonal=p).fit()\n",
    "    results=ets_model.get_forecast(int(n_test))\n",
    "    return results\n",
    "\n",
    "### Function to define the start and end of consecutive missing data\n",
    "\n",
    "def get_nan_inds(series):\n",
    "    series=series.reset_index(drop=True)\n",
    "    index=series[series.isna()].index.to_numpy()\n",
    "    if len(index)==0:\n",
    "        return []\n",
    "    indices=np.split(index,np.where(np.diff(index)>1)[0]+1)\n",
    "    return [(ind[0],ind[-1]+1) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=glob.glob(pathr+\"/*.csv\")\n",
    "names=[\"\" for i in range(len(files))] # names of sites.\n",
    "for i in range(len(files)):\n",
    "    names[i]=files[i].split('/')[len(files[i].split('/'))-1].split('.')[0]\n",
    "\n",
    "for ii in range(len(names)):\n",
    "#for ii in range(0,1):\n",
    "    info_db=[] # dataframe of statistic info of nan filling.\n",
    "    dci_db=[]  # dataframe of confidence intervals info of nan filling.\n",
    "    print('i='+str(ii),names[ii])\n",
    "    start_run=datetime.now()\n",
    "    w=names.index(names[ii])\n",
    "    data=pd.read_csv(files[w],skiprows=11)\n",
    "    line=open(files[w], \"r\").readlines()[0:8]\n",
    "    site=line[0].split(',')[1].strip()\n",
    "    code=line[1].split(',')[1].strip()\n",
    "    country=line[2].split(',')[1].strip()\n",
    "    latitude=line[3].split(',')[1].strip()\n",
    "    longitude=line[4].split(',')[1].strip()\n",
    "    altitude=line[5].split(',')[1].strip()\n",
    "    units=line[6].split(',')[1].strip()\n",
    "    nHeights=int(line[7].split(',')[1].strip())\n",
    "    \n",
    "    titles=data.columns.values.tolist()\n",
    "    titles2=data.columns[7:7+nHeights] # columns of data for each height.\n",
    "    data[\"date\"]=pd.to_datetime(data[\"date\"])\n",
    "    data['doy']=data['date'].dt.dayofyear\n",
    "    \n",
    "### Replace missing data with NaN\n",
    "    data=data.replace(-999.0,np.NaN)\n",
    "    data=data.replace(-999.99,np.NaN)\n",
    "    for i in range(nHeights):\n",
    "        data[titles2[i]][data[titles2[i]]<0]=np.NaN\n",
    "    \n",
    "### Identify time resolution\n",
    "    w=data['date'][1].minute-data['date'][0].minute\n",
    "    if w==30:\n",
    "        dt=0.5 #delta of time [d].\n",
    "    elif w==0:\n",
    "        dt=1\n",
    "    else:\n",
    "        print('dt is not 1 h or 0.5 h')\n",
    "    \n",
    "    maxf=24/dt # maximum length to fill (1 day=48 or 24 values). ***\n",
    "    minb=3*24/dt # minimum length of values before the missing value (3 days=144 or 72). ***\n",
    "    \n",
    "    p_miss0=np.zeros(nHeights); p_missf=np.zeros(nHeights) # percentage of missing data before and after filling.\n",
    "    for jj in range(nHeights):\n",
    "    #for jj in range(1):\n",
    "        count=0;count2=0\n",
    "        print(titles2[jj])\n",
    "        series=data[titles2[jj]]\n",
    "        \n",
    "### Define the start and end of consecutive missing data\n",
    "        nan_index=np.array(get_nan_inds(series)) # start and end of consective nan data.\n",
    "        nan_indexf=nan_index # vector with consecutive nan data positions that are not filled.\n",
    "        if nan_index[0][0]!=0: \n",
    "            nan_indexf=np.insert(nan_indexf,0,[[0]],axis=0)\n",
    "        series=np.array(series)\n",
    "        info=[] # array to write the statistics of the procedure to fill in the missing data.\n",
    "        dci=[] # array to write the amplitude of the confidence interval and the mean SE.\n",
    "        \n",
    "        for i in range(nan_index.shape[0]):\n",
    "            lenf=nan_index[i][1]-nan_index[i][0]\n",
    "            #print(lenf)\n",
    "            if lenf<=maxf:\n",
    "                if nan_index[i][0]!=0: # if there are values before.\n",
    "                    w=np.where([nan_indexf[:,1]<nan_index[i][0]])[1].max()\n",
    "                    lenb=nan_index[i][0]-nan_indexf[w,1]\n",
    "                    print(lenb)\n",
    "                    if lenb>minb:\n",
    "                        count=count+1\n",
    "                        print(f\"{i}/{nan_index.shape[0]}_{names[ii]}\")\n",
    "                        w2=np.where([nan_indexf==nan_index[i]])[1][0]\n",
    "                        nan_indexf=np.delete(nan_indexf,w2,0)\n",
    "                        dataf=series[nan_indexf[w,1]:nan_index[i][0]]\n",
    "### Define the best model describing the previous part of the series.                \n",
    "                        n_test=lenf\n",
    "                        cfg_list=exp_smoothing_configs(seasonal=[24/dt])\n",
    "                        scores=grid_search(dataf,cfg_list,n_test)\n",
    "                        for cfg,error,r2,aa,bb,gg,ff in scores[:1]:\n",
    "                            print(cfg,error,r2,aa,bb,gg,ff) \n",
    "                            info_fill=[i,nan_indexf[w,1],nan_index[i][0],lenf,error,r2]\n",
    "                            info.append(info_fill)\n",
    "                        \n",
    "### Using the best model, fill the missing data.  \n",
    "                        cfg=ast.literal_eval(scores[0][0])\n",
    "                        dataf2=series[nan_indexf[w,1]:nan_index[i][1]]\n",
    "                        fill=np.array(walk_forward_validation_f(dataf2,n_test,cfg))\n",
    "                        series[nan_index[i][0]:nan_index[i][1]]=fill[fill.size-n_test:] \n",
    "### If the model is additive, it finds the confidence interval for each filled value.\n",
    "                        if cfg[0]!='mul' and cfg[2]!='mul':\n",
    "                            count2=count2+1\n",
    "                            dataf2=series[nan_indexf[w,1]:nan_index[i][1]]\n",
    "                            confidence=ci_additive(dataf2,cfg,n_test)\n",
    "                            d_ci=confidence.summary_frame(alpha=0.05)[[\"mean\"]].values-confidence.summary_frame(alpha=0.05)[[\"mean_ci_lower\"]].values # amplitude of the confidence interval.\n",
    "                            mean_se=confidence.summary_frame(alpha=0.05)[[\"mean_se\"]].values\n",
    "                            for l in range(n_test):\n",
    "                                x=[i,float(d_ci[l]),float(mean_se[l])]\n",
    "                                dci.append(x)\n",
    "        if jj==0:\n",
    "            dci_db=pd.DataFrame(dci,columns=['id_'+titles2[jj],'dci_'+titles2[jj],\n",
    "                                                                    'se_'+titles2[jj]])\n",
    "        else:\n",
    "            df=pd.DataFrame(dci,columns=['id_'+titles2[jj],'dci_'+titles2[jj],\n",
    "                    'se_'+titles2[jj]])\n",
    "        if jj>0:\n",
    "            dci_db=pd.concat([dci_db,df],ignore_index=False,axis=1)                                \n",
    "### Save statistics in a dataframe \n",
    "# initial (t0) and final (tf) times considered to select the best model\n",
    "# Lnan is the length of the consecutive nan data filled, MSE is the error, r2)        \n",
    "        if count>0:\n",
    "            if jj==0:\n",
    "                info_db=pd.DataFrame(info,\n",
    "                columns=['id_'+titles2[jj],'t0_'+titles2[jj],'tf_'+titles2[jj],'Lnan_'+titles2[jj],\n",
    "                         'MSE_'+titles2[jj],'r2_'+titles2[jj]])\n",
    "            else:\n",
    "                df2=pd.DataFrame(info,\n",
    "                columns=['id_'+titles2[jj],'t0_'+titles2[jj],'tf_'+titles2[jj],'Lnan_'+titles2[jj],\n",
    "                         'MSE_'+titles2[jj],'r2_'+titles2[jj]])\n",
    "            if jj>0:\n",
    "                info_db=pd.concat([info_db,df2],ignore_index=False,axis=1)    \n",
    "### Save filled data in a new column of the dataframe\n",
    "        data[titles2[jj]+\"_fill\"]=series\n",
    "        p_miss0[jj]=data[titles2[jj]].isnull().sum()/data.shape[0]*100\n",
    "        p_missf[jj]=data[titles2[jj]+\"_fill\"].isnull().sum()/data.shape[0]*100\n",
    "        print(p_miss0[jj],p_missf[jj])\n",
    "### Save the filled dataframe in a new file.\n",
    "    header='Site,'+site+'\\nCode,'+code+'\\nCountry,'+country+'\\nLatitude,'+str(latitude)+'\\nLongitude,'+str(longitude)+'\\nAltitude,'+str(altitude)+'\\nUnits,'+units+'\\nnHeights,'+str(nHeights)+'\\n'+'\\n'\n",
    "    with open(paths+code+'_filled.csv', 'w') as fp:\n",
    "        fp.write(header)\n",
    "    data.to_csv(paths+code+'_filled.csv',header=True,mode='a')            \n",
    "### Save the statistics info dataframe in a new file.\n",
    "    if len(info_db)!=0:\n",
    "        info_db=info_db.append({'id_'+titles2[0]:'' },ignore_index=True)\n",
    "        info_db=info_db.append({'id_'+titles2[0]:'Height','t0_'+\n",
    "                            titles2[0]:'p before','tf_'+titles2[0]:'p after'},ignore_index=True)\n",
    "        for l in range(nHeights):\n",
    "            info_db=info_db.append({'id_'+titles2[0]:titles2[l],'t0_'+titles2[0]:p_miss0[l],\n",
    "                                'tf_'+titles2[0]:p_missf[l]},ignore_index=True)\n",
    "        info_db.to_csv(paths+code+'_filled_info.csv',header=True)    \n",
    "### Save the confidence interval info dataframe in a new file.\n",
    "    if len(dci_db)!=0:\n",
    "        dci_db.to_csv(paths+code+'_filled_ci.csv',header=True) \n",
    "    end_run=datetime.now()   \n",
    "    print(f\"Runtime of the program is {end_run-start_run}\") \n",
    "        \n",
    "        \n",
    "# *** values to define."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
