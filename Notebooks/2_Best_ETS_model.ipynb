{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ETS model to decompose the CO$_2$ time series.\n",
    "\n",
    "This notebook selects the best 5 ETS models decomposing time series of each site and selects the most frequent. Then, it creates .csv files with the results of each site and considering all sites together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ast\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from math import sqrt\n",
    "from datetime import *\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathr='/home/_ehoyos/Documents/Data_CO2/Final_info/' # path to read the data.\n",
    "paths='/home/_ehoyos/Documents/Data_CO2/Final_results/' # path to save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to select the best models\n",
    "def exp_smoothing_forecast_b(history,config):\n",
    "    t,d,s,p,r,n=config\n",
    "    # define model\n",
    "    history=array(history)\n",
    "    model=ExponentialSmoothing(history,trend=t,damped_trend=d,seasonal=s,seasonal_periods=p)\n",
    "    # fit model\n",
    "    model_fit=model.fit(optimized=True,remove_bias=r)\n",
    "    yhat=model_fit.predict(0,len(history)-1) # modified line.\n",
    "    \n",
    "    # estimate prediction error\n",
    "    error,r2=measure_rmse(history,yhat) # added line\n",
    "    return (error,r2,model_fit.params[\"smoothing_level\"],model_fit.params[\"smoothing_trend\"],\n",
    "           model_fit.params[\"smoothing_seasonal\"],model_fit.params[\"damping_trend\"])\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    error=sqrt(mean_squared_error(actual, predicted))\n",
    "    r2=metrics.r2_score(actual,predicted) # I added this line.\n",
    "    return (error,r2)\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation_b(dataf2,cfg):\n",
    "    train=dataf2 ; test=dataf2\n",
    "    history=[x for x in train]\n",
    "    error,r2,a1,b1,g1,f1=exp_smoothing_forecast_b(history,cfg) # modified line.\n",
    "    return (error,r2,a1,b1,g1,f1)\n",
    "\n",
    "# score a model, return None on failure\n",
    "def score_model_b(dataf2,cfg,debug=False):\n",
    "    result=None\n",
    "    r2=None\n",
    "    aa=None;bb=None;gg=None;ff=None #values of alpha, beta, gamma and phi.\n",
    "    # convert config to a key\n",
    "    key=str(cfg)\n",
    "    # show all warnings and fail on exception if debugging\n",
    "    if debug:\n",
    "        result,r2,aa,bb,gg,ff=walk_forward_validation(dataf2,cfg)\n",
    "    else:\n",
    "        # one failure during model validation suggests an unstable config\n",
    "        try:\n",
    "            # never show warnings when grid searching, too noisy\n",
    "            with catch_warnings():\n",
    "                filterwarnings(\"ignore\")\n",
    "                result,r2,aa,bb,gg,ff=walk_forward_validation_b(dataf2,cfg)\n",
    "        except:\n",
    "            error=None\n",
    "    return (key,result,r2,aa,bb,gg,ff)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search_b(dataf,cfg_list,parallel=True):\n",
    "    scores=None\n",
    "    if parallel:\n",
    "        # execute configs in parallel\n",
    "        executor=Parallel(n_jobs=cpu_count(),backend='multiprocessing')\n",
    "        tasks=(delayed(score_model_b)(dataf2,cfg) for cfg in cfg_list)\n",
    "        scores=executor(tasks)\n",
    "    else:\n",
    "        scores=[score_model_b(dataf2,cfg) for cfg in cfg_list]\n",
    "    # remove empty results\n",
    "    scores=[r for r in scores if r[1] !=None]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup:tup[1])\n",
    "    return scores\n",
    "\n",
    "# create a set of exponential smoothing configs to try\n",
    "def exp_smoothing_configs_b(seasonal=[None]):\n",
    "    models=list()\n",
    "    c=0\n",
    "    # define config lists\n",
    "    t_params=['add','mul',None] # trend.\n",
    "    d_params=[True,False] # damped trend.\n",
    "    s_params=['add','mul',None] # seasonal.\n",
    "    p_params=seasonal # seasonal period.\n",
    "    r_params=[True,False] # remove bias.\n",
    "    # create config instances\n",
    "    for t in t_params:\n",
    "        for d in d_params:\n",
    "            for s in s_params:\n",
    "                for p in p_params:\n",
    "                    for r in r_params:\n",
    "                        c=c+1\n",
    "                        cfg=[t,d,s,p,r,c]\n",
    "                        models.append(cfg)\n",
    "    return models\n",
    "\n",
    "### Function to define the start and end of consecutive missing data\n",
    "\n",
    "def get_nnan_inds(series):\n",
    "    series=series.reset_index(drop=True)\n",
    "    index=series[series.notna()].index.to_numpy()\n",
    "    if len(index)==0:\n",
    "        return []\n",
    "    indices=np.split(index,np.where(np.diff(index)>1)[0]+1)\n",
    "    return [(ind[0],ind[-1]+1,ind[-1]+1-ind[0]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=46 HEL\n",
      "CO2_110.0_fill\n",
      "1 0/1\n",
      "[ 1  2  3  4 21]\n"
     ]
    }
   ],
   "source": [
    "filesf=glob.glob(pathr+\"/*.csv\")\n",
    "namesf=[\"\" for i in range(len(filesf))] # names of sites.\n",
    "for i in range(len(filesf)):\n",
    "    namesf[i]=filesf[i].split('/')[len(filesf[i].split('/'))-1].split('.')[0].split('_')[0]\n",
    "\n",
    "bbm_s=[] # array to save the best 5 models describing each site.\n",
    "info_bbm=[]  \n",
    "\n",
    "for ii in range(len(namesf)):\n",
    "#for ii in range(46,47):\n",
    "    bm_s=[] # array with all the best models describing the ii site.\n",
    "    info_model=[]\n",
    "    print('i='+str(ii),namesf[ii])\n",
    "    start_run=datetime.now()\n",
    "    w=namesf.index(namesf[ii])\n",
    "    dataf=pd.read_csv(filesf[w],skiprows=11)\n",
    "    line=open(filesf[w], \"r\").readlines()[0:8]\n",
    "    code=line[1].split(',')[1].strip()\n",
    "    dataf[\"date\"]=pd.to_datetime(dataf[\"date\"])\n",
    "    nHeights=int(line[7].split(',')[1].strip())\n",
    "    titles2=dataf.columns[7:7+nHeights] # columns of data for each height.\n",
    "    titlesf=dataf.columns[7+2*nHeights:7+2*nHeights+nHeights]\n",
    "    \n",
    "### time resolution\n",
    "    w=dataf['date'][1].minute-dataf['date'][0].minute\n",
    "    if w==30:\n",
    "        dt=0.5 #delta of time [d].\n",
    "    elif w==0:\n",
    "        dt=1\n",
    "    else:\n",
    "        print('dt is not 1 h or 0.5 h')\n",
    "    minb=3*24/dt # minimum lenght to analyze.\n",
    "    \n",
    "    for jj in range(nHeights):\n",
    "        print(titlesf[jj])\n",
    "        series=dataf[titlesf[jj]]\n",
    "        nnan_index=np.array(get_nnan_inds(series)) # start, end and lenght of consective non nan data.\n",
    "        c=0; c1=-1\n",
    "        for i in range(nnan_index.shape[0]): \n",
    "            c1=c1+1\n",
    "            if nnan_index[i,2]>=minb:\n",
    "                c=c+1# number of the decomposed portion.\n",
    "                print(c,str(c1)+'/'+str(nnan_index.shape[0]))\n",
    "                dataf2=series[nnan_index[i,0]:nnan_index[i,1]]\n",
    "                cfg_list=exp_smoothing_configs_b(seasonal=[24/dt])\n",
    "                scores=grid_search_b(dataf2,cfg_list)\n",
    "                bm=[] # array with the number of the best models.\n",
    "                for j in range(5):\n",
    "                    bm.append(ast.literal_eval(scores[j][0])[5]) \n",
    "                info=np.append([titles2[jj],c,nnan_index[i,0],nnan_index[i,1],nnan_index[i,2]],bm)\n",
    "                info_model.append(info)\n",
    "                bm_s.append(bm)\n",
    "\n",
    "### save the best 5 models of each part of the decomposed time series in a .csv file\n",
    "    info_model_db=pd.DataFrame(info_model,\n",
    "            columns=['Height','n','t0','tf','Length','m1','m2','m3','m4','m5'])\n",
    "    info_model_db.to_csv(paths+code+'_best_ETSmodels.csv',header=True)\n",
    "\n",
    "### select and save the best 5 models of each site and save them in a .csv file.\n",
    "# the last line contains the best 5 models considerig all sites.\n",
    "    bm_s=np.array(bm_s)\n",
    "    values,counts=np.unique(bm_s,return_counts=True)\n",
    "    if values.size>5:\n",
    "        ind=np.argsort(-1*counts)[:5]\n",
    "        val=values[ind]\n",
    "    else:\n",
    "        val=values\n",
    "    \n",
    "    bbm_s.append(val)\n",
    "    info=np.append([code],val)\n",
    "    info_bbm.append(info)\n",
    "    \n",
    "## 5 best models considering all sites.\n",
    "values,counts=np.unique(bbm_s,return_counts=True)\n",
    "if values.size>5:\n",
    "    ind=np.argsort(-1*counts)[:5]\n",
    "    best=values[ind]\n",
    "    count=counts[ind]\n",
    "else:\n",
    "    best=values\n",
    "    count=counts\n",
    "print(best)\n",
    "\n",
    "info=np.append(['all sites'],best)\n",
    "info_bbm.append(info)\n",
    "info=np.append(['count'],count)\n",
    "info_bbm.append(info)\n",
    "\n",
    "best_models_db=pd.DataFrame(info_bbm,\n",
    "            columns=['Site','m1','m2','m3','m4','m5'])\n",
    "best_models_db.to_csv(paths+'zBest_ETSmodels_all.csv',header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
