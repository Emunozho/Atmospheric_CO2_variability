{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition of the CO$_2$ time series.\n",
    "\n",
    "This notebook descompones the CO$_2$ time series with the best model selected in '2_Best_ETS_model' notebook and extracts the residuals. It creates new .csv files with the time series of the deterministic and stochastic components at each height.\n",
    "\n",
    "Best model: cfg=['add',True,'mul',xx,True, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ast\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from math import sqrt\n",
    "from datetime import *\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathr='/home/_ehoyos/Documents/Data_CO2/Final_info/' # path to read the data.\n",
    "paths='/home/_ehoyos/Documents/Data_CO2/Final_results/' # path to save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_smoothing_forecast_r(history,config):\n",
    "    t,d,s,p,r,n=config\n",
    "    history=array(history)\n",
    "    model=ExponentialSmoothing(history,trend=t,damped_trend=d,seasonal=s,seasonal_periods=p)\n",
    "    model_fit=model.fit(optimized=True,remove_bias=r)\n",
    "    yhat=model_fit.predict(0,len(history)-1)\n",
    "    return yhat\n",
    "\n",
    "def walk_forward_validation_r(dataf2,cfg):\n",
    "    predictions=list()\n",
    "    train=dataf2\n",
    "    history=[x for x in train]\n",
    "    yhat=exp_smoothing_forecast_r(history,cfg)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    return yhat\n",
    "def get_nnan_inds(series):\n",
    "    series=series.reset_index(drop=True)\n",
    "    index=series[series.notna()].index.to_numpy()\n",
    "    if len(index)==0:\n",
    "        return []\n",
    "    indices=np.split(index,np.where(np.diff(index)>1)[0]+1)\n",
    "    return [(ind[0],ind[-1]+1,ind[-1]+1-ind[0]) for ind in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=46 HEL\n",
      "CO2_110.0(1/1)\n",
      "0/1\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "filesf=glob.glob(pathr+\"/*.csv\")\n",
    "namesf=[\"\" for i in range(len(filesf))] # names of ICOS sites.\n",
    "for i in range(len(filesf)):\n",
    "    namesf[i]=filesf[i].split('/')[len(filesf[i].split('/'))-1].split('.')[0].split('_')[0]\n",
    "\n",
    "for ii in range(len(namesf)):\n",
    "#for ii in range(46,47):\n",
    "    print('i='+str(ii),namesf[ii])\n",
    "    w=namesf.index(namesf[ii])\n",
    "    dataf=pd.read_csv(filesf[w],skiprows=11)\n",
    "    line=open(filesf[w], \"r\").readlines()[0:8]\n",
    "    code=line[1].split(',')[1].strip()\n",
    "    site=line[0].split(',')[1].strip()\n",
    "    code=line[1].split(',')[1].strip()\n",
    "    country=line[2].split(',')[1].strip()\n",
    "    latitude=line[3].split(',')[1].strip()\n",
    "    longitude=line[4].split(',')[1].strip()\n",
    "    altitude=line[5].split(',')[1].strip()\n",
    "    units=line[6].split(',')[1].strip()\n",
    "    nHeights=int(line[7].split(',')[1].strip())\n",
    "    dataf[\"date\"]=pd.to_datetime(dataf[\"date\"])\n",
    "    titlesf=dataf.columns[7+2*nHeights:7+2*nHeights+nHeights]\n",
    "    titles2=dataf.columns[7:7+nHeights]\n",
    "    \n",
    "### time resolution\n",
    "    w=dataf['date'][1].minute-dataf['date'][0].minute\n",
    "    if w==30:\n",
    "        dt=0.5 #delta of time [d].\n",
    "        xx=48.\n",
    "    elif w==0:\n",
    "        dt=1\n",
    "        xx=24.\n",
    "    else:\n",
    "        print('dt is not 1 h or 0.5 h')\n",
    "    \n",
    "    cfg=['add',True,'mul',xx,True, 3]# Best model identified previously*.\n",
    "    \n",
    "# descompose the time series and extract the residuals.    \n",
    "    model=np.empty((len(dataf))) # array with the modeled values (trend and seasonality).\n",
    "    residuals_add=np.empty((len(dataf))) # array with the additive residuals of each site.\n",
    "    residuals_mul=np.empty((len(dataf))) # array with the additive residuals of each site.\n",
    "    determin=np.empty((len(dataf))) # array with the deterministic components of each site (trend and seasonality).\n",
    "    residuals_add[:]=np.nan\n",
    "    residuals_mul[:]=np.nan\n",
    "    model[:]=np.nan\n",
    "    determin[:]=np.nan\n",
    "    for jj in range(nHeights):\n",
    "        print(titles2[jj]+'('+str(jj+1)+'/'+str(nHeights)+')')\n",
    "        series=dataf[titlesf[jj]]\n",
    "        nnan_index=np.array(get_nnan_inds(series)) # start, end and lenght of consective non nan data.\n",
    "        for i in range(nnan_index.shape[0]): \n",
    "            print(str(i)+'/'+str(nnan_index.shape[0]))\n",
    "            if nnan_index[i,1]-nnan_index[i,0] >= 2*24/dt: # the method requires at least 2 periods.\n",
    "                dataf2=series[nnan_index[i,0]:nnan_index[i,1]]\n",
    "                fill=np.array(walk_forward_validation_r(dataf2,cfg))\n",
    "                resid=dataf2-fill\n",
    "                residuals_add[nnan_index[i,0]:nnan_index[i,1]]=resid\n",
    "                resid=(dataf2-fill)/fill\n",
    "                residuals_mul[nnan_index[i,0]:nnan_index[i,1]]=resid\n",
    "                determin[nnan_index[i,0]:nnan_index[i,1]]=fill\n",
    "        dataf[titles2[jj]+\"_resid\"]=residuals_add\n",
    "        dataf[titles2[jj]+\"_deterministic\"]=determin  \n",
    "    header='Site,'+site+'\\nCode,'+code+'\\nCountry,'+country+'\\nLatitude,'+str(latitude)+'\\nLongitude,'+str(longitude)+'\\nAltitude,'+str(altitude)+'\\nUnits,'+units+'\\nnHeights,'+str(nHeights)+'\\n'+'\\n'\n",
    "    with open(paths+code+'_decomposed.csv', 'w') as fp:\n",
    "        fp.write(header)\n",
    "    dataf.to_csv(paths+code+'_decomposed.csv',header=True,mode='a')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
